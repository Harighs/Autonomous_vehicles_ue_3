{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "Our project's objective is to achieve precise localization for a car driving within a simulated environment. The task involves ensuring that the car can travel a minimum distance of 170 meters from its starting position while minimizing the distance pose error. Practically, this error should not exceed 1.2 meters.\n",
    "\n",
    "To accomplish this, we will utilize point clouds extracted from a simulated car equipped with a LIDAR sensor, which provides regular LIDAR scans. Additionally, we have an existing point cloud map, which has been extracted from the CARLA simulator. Localization will be achieved by performing point registration matching between the real-time LIDAR scans and the pre-existing point cloud map. For this we implemented the ICP and NTD localization algorithm respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "TODO: Write explaination about the steps to run the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICP Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iterative Closest Point (ICP) algorithm is used to align two sets of points, typically referred to as the source points \\( P \\) and the target points \\( Q \\). In the context of a self-driving car, the target points \\( Q \\) are provided by a map, while the source points \\( P \\) come from the car's sensors. The goal is to transform the car's sensor data so it aligns with the map coordinates, specifically aligning the x and y coordinates through translation and rotation. This is achieved by minimizing the sum of squared errors between the points. This is achieved iteratively. \n",
    "\n",
    "One iteration includes the following steps:\n",
    "\n",
    "1. Calculate the centroids (mean points) of both \\( P \\) and \\( Q \\).\n",
    "\n",
    "2. Create a new set \\( X \\) by subtracting each point in \\( P \\) by the centroid of \\( P \\), and create a new set \\( Y \\) by subtracting each point in \\( Q \\) by the centroid of \\( Q \\).\n",
    "\n",
    "3. Compute the cross covariance matrix \\( S \\) by multiplying \\( X \\) with the transpose of \\( Y \\).\n",
    "\n",
    "4. Perform SVD on the matrix \\( S \\) to obtain matrices \\( U \\), \\( \\Sigma \\), and \\( V^T \\).\n",
    "\n",
    "5. \n",
    "    - The rotation matrix \\( R \\) is calculated as \\( R = VU^T \\).\n",
    "    - The translation vector \\( t \\) is calculated as \\( t = \\mu_Q - R\\mu_P \\), where \\( \\mu_Q \\) and \\( \\mu_P \\) are the centroids of \\( Q \\) and \\( P \\), respectively.\n",
    "\n",
    "6. We then iterativeley repeat the above steps until the sum of squared errors is minimized to a satisfactory level.\n",
    "\n",
    "By iteratively updating the transformation, the ICP algorithm aligns the source points \\( P \\) to the target points \\( Q \\), effectively mapping the car's sensor data to the map coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module configuration\n",
    "\n",
    "TODO: Explain the module configuration and selected parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset performance metrics\n",
    "\n",
    "On the provided dataset, our model demonstrated the following performance metrics:\n",
    "\n",
    "- **Mean time per frame**: 0.12 seconds\n",
    "- **Maximal time per frame**: 0.59 seconds\n",
    "- **Mean lateral error**: 0.07 meters\n",
    "- **Maximal lateral error**: 0.07 meters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update paths, remove final part about visualization?\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#\n",
    "# Paths\n",
    "map_path = \"localization/dataset/map.pcd\"\n",
    "frames_path = \"localization/dataset/frames\"\n",
    "ground_truth_path = \"localization/dataset/ground_truth.csv\"\n",
    "\n",
    "# point cloud\n",
    "map_cloud = o3d.io.read_point_cloud(map_path)\n",
    "\n",
    "# ground truth\n",
    "ground_truth = pd.read_csv(ground_truth_path)\n",
    "\n",
    "extracted_cars = []\n",
    "\n",
    "#frame\n",
    "for i in range(len(ground_truth)):\n",
    "    frame_file = os.path.join(frames_path, f\"frame_{i}.pcd\")\n",
    "\n",
    "    if not os.path.exists(frame_file):\n",
    "        print(f\"Frame file {frame_file} not found.\")\n",
    "        continue\n",
    "\n",
    "    frame_cloud = o3d.io.read_point_cloud(frame_file)\n",
    "    frame_cloud_down = frame_cloud.voxel_down_sample(voxel_size=0.05)\n",
    "\n",
    "    # ground truth\n",
    "    x, y, z = ground_truth.iloc[i][1:4]\n",
    "    roll, pitch, yaw = np.deg2rad(ground_truth.iloc[i][4:7])\n",
    "\n",
    "    # transformation matrix\n",
    "    r = R.from_euler('xyz', [roll, pitch, yaw])\n",
    "    rotation_matrix = r.as_matrix()\n",
    "    translation_vector = np.array([x, y, z])\n",
    "    transformation_matrix = np.eye(4)\n",
    "    transformation_matrix[:3, :3] = rotation_matrix\n",
    "    transformation_matrix[:3, 3] = translation_vector\n",
    "\n",
    "    # transformation\n",
    "    frame_cloud_down.transform(transformation_matrix)\n",
    "    extracted_cars.append(frame_cloud_down)\n",
    "\n",
    "# ICP\n",
    "threshold = 0.1\n",
    "initial_transformation = np.eye(4)\n",
    "maximum_iteration = 100\n",
    "\n",
    "time_rec = []\n",
    "lateral_err = []\n",
    "\n",
    "\n",
    "for i, car_cloud in enumerate(extracted_cars):\n",
    "    start_time = time.time()\n",
    "\n",
    "    reg_p2l = o3d.pipelines.registration.registration_icp(\n",
    "        car_cloud, map_cloud, threshold, initial_transformation,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=maximum_iteration)\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_rec.append(end_time - start_time)\n",
    "\n",
    "    # registration error\n",
    "    lateral_error = o3d.pipelines.registration.evaluate_registration(\n",
    "        car_cloud, map_cloud, threshold, reg_p2l.transformation\n",
    "    ).inlier_rmse\n",
    "    lateral_err.append(lateral_error)\n",
    "\n",
    "    if lateral_error > 1.2:\n",
    "        print(f\"Lateral error ({lateral_error:.2f} m) is greater than the maximum allowed (1.2 m).\")\n",
    "        break\n",
    "\n",
    "    # initial transformation\n",
    "    initial_transformation = reg_p2l.transformation\n",
    "\n",
    "    print(f\"Processed {i + 1}/{len(extracted_cars)} frames.\")\n",
    "\n",
    "# results\n",
    "print(f\"----->>>>>>>>>>  Mean time per frame: {np.mean(time_rec):.2f} s\")\n",
    "print(f\"----->>>>>>>>>>  Maximal time per frame: {np.max(time_rec):.2f} s\")\n",
    "print(f\"----->>>>>>>>>>  Mean lateral error: {np.mean(lateral_err):.2f} m\")\n",
    "print(f\"----->>>>>>>>>>  Maximal lateral error: {np.max(lateral_err):.2f} m\")\n",
    "\n",
    "\n",
    "# aligned car point clouds\n",
    "merged_cloud = map_cloud\n",
    "for car_cloud in extracted_cars:\n",
    "    merged_cloud += car_cloud\n",
    "\n",
    "o3d.io.write_point_cloud(\"aligned_car.pcd\", merged_cloud)\n",
    "# # ######################################################### End of solution #########################################################\n",
    "\n",
    "# After saving the aligned point cloud, you can visualize it using the following code:\n",
    "# pcd = o3d.io.read_point_cloud(\"/home/ari/Workplace/JKU/SEM_2/Autonomous_sys/Project3/Localization_Project/localization/Project/ICP/aligned_car.pcd\")\n",
    "# o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTD Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm explaination\n",
    "\n",
    "Unlike the Iterative Closest Point (ICP) algorithm, which minimizes point-to-point distances, Normal Distribution Transform (NDT) aligns a set of source points to a target point cloud by modeling the target as a set of normal distributions via Multivariate Normal Distribution. Since it is possible that we obtain probability density functions that dont represent our point cloud, we divide the space of the poinclouds into a grid of cells (also called voxels) and calculate the Multivariate Normal Distribution for each cell. \n",
    "\n",
    " #### TODO: Explain the steps in NTD\n",
    " \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialization: \n",
    "Before the alignment , the algorithm initializes parameters such as grid size and point cloud resolution. This is where the NDT class and other helper classes like Pose and Cell are defined to support the main NDT operations.\n",
    "\n",
    "2. Preprocessing:\n",
    "The input cloud is set into the NDT grid using set_input_cloud. Here, the point cloud data is divided into a 3D grid of cells. The grid size is determined by the resolution parameter. The input cloud is then transformed into the grid coordinates.\n",
    "\n",
    "3. Gaussian Distribution Calculation:\n",
    "Each cell calculates and stores a mean and covariance matrix representing the Gaussian distribution of points within that cell.\n",
    "\n",
    "4. The target cloud (to be aligned) is iteratively transformed and aligned to the reference model built from the input cloud.For each iteration, the algorithm computes a transformation matrix from the current pose estimation.The transformed point cloud is then used to calculate a score indicating how well it fits the reference model.A Newton-Raphson update step is applied to refine the pose estimation, aiming to maximize the alignment score.\n",
    "\n",
    "5. Iteration and Convergence:The alignment iterates, updating the pose based on calculated gradients and the Hessian matrix until convergence criteria are met (based on a threshold or a maximum number of iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module configuration\n",
    "TODO: Explain the module configuration and selected parameters \n",
    "\n",
    "\n",
    "x_step, y_step, z_step: The resolution of the grid in each dimension, set to 15 units in your case. This resolution defines the size of each cell in the 3D grid.\n",
    "\n",
    "eps: The convergence threshold, set to 1e-3, determining the minimum change between iterations for the process to be considered as converged.\n",
    "\n",
    "In the implementation, after setting up the NDT environment with the defined grid size and resolution, we proceed to align a series of extracted car point clouds against a map. The initial pose is assumed to be neutral (Pose(0, 0, 0, 0, 0, 0)), and this iteratively refine this pose based on the NDT alignment results. The lateral error and timing are tracked to evaluate the performance.\n",
    "\n",
    "In the begining since we didn't have the right configuration for NDT from open3d. we tried to implement the NDT algorithm from scratch. We used the following steps to implement the NDT algorithm from scratch:\n",
    "\n",
    "1. Initialize the NDT grid with the given resolution and grid size.\n",
    "2. Preprocess the input point cloud by transforming it into the grid coordinates.\n",
    "3. Calculate the Gaussian distribution for each cell in the grid.\n",
    "4. Align the target cloud to the reference model iteratively.\n",
    "5. Update the pose estimation based on the calculated gradients and Hessian matrix.\n",
    "6. Repeat the alignment process until convergence criteria are met.\n",
    "\n",
    "but somehow we were not able to get the right results. the threshold for the lateral error was not met. it was increasesing incrementally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lateral error is : 0.5159613500514074\n",
    "Processed 949/1014 frames.\n",
    "Lateral error is : 0.48686348588548367\n",
    "Processed 950/1014 frames.\n",
    "Lateral error is : 0.4907630302487757\n",
    "Processed 951/1014 frames.\n",
    "Lateral error is : 0.542017974444391\n",
    "Processed 952/1014 frames.\n",
    "Lateral error is : 0.5516454626605112\n",
    "Processed 953/1014 frames.\n",
    "Lateral error is : 0.5567147416256384\n",
    "Processed 954/1014 frames.\n",
    "Lateral error is : 0.5738285602910101\n",
    "Processed 955/1014 frames.\n",
    "Lateral error is : 0.611308163787111\n",
    "Processed 956/1014 frames.\n",
    "Lateral error is : 0.5971138202110213\n",
    "Processed 957/1014 frames.\n",
    "Lateral error is : 0.6755034481165051\n",
    "Processed 958/1014 frames.\n",
    "Lateral error is : 0.6468705750935433\n",
    "Processed 959/1014 frames.\n",
    "Lateral error is : 0.7296657503271184\n",
    "Processed 960/1014 frames.\n",
    "Lateral error is : 0.6938888359361489\n",
    "Processed 961/1014 frames.\n",
    "Lateral error is : 0.8619496501455042\n",
    "Processed 962/1014 frames.\n",
    "Lateral error is : 0.8766180618122889\n",
    "Processed 963/1014 frames.\n",
    "Lateral error is : 0.8429278811469212\n",
    "Processed 964/1014 frames.\n",
    "Lateral error is : 1.014644693568059\n",
    "Processed 965/1014 frames.\n",
    "Lateral error is : 1.0259737317016615\n",
    "Processed 966/1014 frames.\n",
    "Lateral error is : 0.9943898462532704\n",
    "Processed 967/1014 frames.\n",
    "Lateral error is : 1.0945663941821813\n",
    "Processed 968/1014 frames.\n",
    "Lateral error is : 1.1947511112389448\n",
    "Processed 969/1014 frames.\n",
    "Lateral error is : 1.1551949147881855\n",
    "Processed 970/1014 frames.\n",
    "Lateral error is : 1.1829704847569473\n",
    "Processed 971/1014 frames.\n",
    "Lateral error is : 1.3387442052416239\n",
    "Lateral error (1.34 m) is greater than the maximum allowed (1.2 m).\n",
    "\n",
    "we got that we reached the maximum allowed lateral error of 1.2 meters at the frame of 971. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset performance metrics\n",
    "\n",
    "- **Mean time per frame**: 0.08 s\n",
    "- **Maximal time per frame**: 0.35 s\n",
    "- **Mean lateral error**: 94.98 m\n",
    "- **Maximal lateral error**: 182.41 m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "#### Gaussian Distributions ###\n",
    "def compute_gaussian_distributions(points, voxel_indices):\n",
    "    voxel_dict = {}\n",
    "\n",
    "    for point, index in zip(points, voxel_indices):\n",
    "        index_tuple = tuple(index)\n",
    "        if index_tuple not in voxel_dict:\n",
    "            voxel_dict[index_tuple] = []\n",
    "        voxel_dict[index_tuple].append(point)\n",
    "\n",
    "    voxel_mean = {}\n",
    "    voxel_covariance = {}\n",
    "\n",
    "    for index in voxel_dict:\n",
    "        voxel_points = np.array(voxel_dict[index])\n",
    "\n",
    "        if len(voxel_points) < 2:\n",
    "            continue\n",
    "\n",
    "        q = np.mean(voxel_points, axis=0)\n",
    "        C = np.cov(voxel_points, rowvar=False)\n",
    "        C += np.eye(C.shape[0]) * 1e-6\n",
    "\n",
    "        voxel_mean[index] = q\n",
    "        voxel_covariance[index] = C\n",
    "\n",
    "    return voxel_mean, voxel_covariance\n",
    "\n",
    "### Rotation Matrix from RPY ###\n",
    "def rpy_to_rotation_matrix(roll, pitch, yaw):\n",
    "    # Calculate cosines and sines\n",
    "    cy = np.cos(yaw)\n",
    "    sy = np.sin(yaw)\n",
    "    cp = np.cos(pitch)\n",
    "    sp = np.sin(pitch)\n",
    "    cr = np.cos(roll)\n",
    "    sr = np.sin(roll)\n",
    "\n",
    "    # Create rotation matrices\n",
    "    R_z = np.array([\n",
    "        [cy, -sy, 0],\n",
    "        [sy, cy, 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    R_y = np.array([\n",
    "        [cp, 0, sp],\n",
    "        [0, 1, 0],\n",
    "        [-sp, 0, cp]\n",
    "    ])\n",
    "\n",
    "    R_x = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, cr, -sr],\n",
    "        [0, sr, cr]\n",
    "    ])\n",
    "\n",
    "    # Combine rotations\n",
    "    R = np.dot(R_z, np.dot(R_y, R_x))\n",
    "    return R\n",
    "\n",
    "### Voxelization ###\n",
    "def voxelize(points, voxel_size):\n",
    "    min_bounds = np.min(points, axis=0)\n",
    "    voxel_indices = np.floor((points - min_bounds) / voxel_size).astype(int)\n",
    "    return voxel_indices, min_bounds\n",
    "\n",
    "def compare_transformations(computed, ground_truth):\n",
    "    translation_gt = ground_truth[[' x', ' y', ' z']].values\n",
    "    rpy_gt = ground_truth[[' roll', ' pitch', ' yaw']].values\n",
    "    rotation_gt = rpy_to_rotation_matrix(*rpy_gt)\n",
    "\n",
    "    gt_transform = np.eye(4)\n",
    "    gt_transform[:3, :3] = rotation_gt\n",
    "    gt_transform[:3, 3] = translation_gt\n",
    "\n",
    "    # print(\"Ground Truth Transformation Matrix:\\n\", gt_transform)\n",
    "    # print(\"Difference in Transformation:\\n\", computed - gt_transform)\n",
    "\n",
    "### NDT Registration ###\n",
    "def ndt_registration(source_points, voxel_mean, voxel_covariance, voxel_size, min_bounds, max_iterations=30, tolerance=1e-6):\n",
    "    transformation = np.eye(4)\n",
    "\n",
    "    def get_voxel_index(point):\n",
    "        return tuple(np.floor((point - min_bounds) / voxel_size).astype(int))\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        transformed_points = (transformation[:3, :3] @ source_points.T + transformation[:3, 3].reshape(-1, 1)).T\n",
    "\n",
    "        likelihood = 0\n",
    "        gradient = np.zeros((6,))\n",
    "\n",
    "        for point in transformed_points:\n",
    "            voxel_index = get_voxel_index(point)\n",
    "            if voxel_index in voxel_mean:\n",
    "                q = voxel_mean[voxel_index]\n",
    "                C = voxel_covariance[voxel_index]\n",
    "\n",
    "                diff = point - q\n",
    "                likelihood += np.exp(-0.5 * diff.T @ np.linalg.inv(C) @ diff)\n",
    "                # Compute the gradient based on the likelihood (not fully implemented here)\n",
    "\n",
    "        # Update the transformation using the gradient (simplified, gradient step needs implementation)\n",
    "        # transformation_update = compute_transformation_update(gradient)\n",
    "        # transformation = transformation @ transformation_update\n",
    "\n",
    "        if np.linalg.norm(gradient) < tolerance:\n",
    "            break\n",
    "\n",
    "    return transformation\n",
    "\n",
    "### Load Point Cloud ###\n",
    "def load_point_cloud(filename):\n",
    "    return o3d.io.read_point_cloud(filename)\n",
    "\n",
    "def apply_voxel_downsampling(pcd, voxel_size=0.05):\n",
    "    return pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "\n",
    "def load_ground_truth(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "#### Main ####\n",
    "# Project directory setup\n",
    "\n",
    "# TODO: Change directory set up below\n",
    "\n",
    "# project_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# project_dir = '/'.join(project_dir.split('/')[:-2])\n",
    "project_dir = os.getcwd()\n",
    "dataset_dir = os.path.join(project_dir, 'localization/dataset')\n",
    "frames_dir = sorted(os.listdir(os.path.join(dataset_dir, 'frames')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load map and ground truth\n",
    "map_pcd = load_point_cloud(os.path.join(dataset_dir, 'map.pcd'))\n",
    "map_points = np.asarray(map_pcd.points)\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth = load_ground_truth(os.path.join(dataset_dir, 'ground_truth.csv'))\n",
    "\n",
    "voxel_size = 1.0\n",
    "voxel_indices, min_bounds = voxelize(map_points, voxel_size)\n",
    "voxel_mean, voxel_covariance = compute_gaussian_distributions(map_points, voxel_indices)\n",
    "\n",
    "# Placeholder for initial transformation (identity)\n",
    "initial_transformation = np.eye(4)\n",
    "\n",
    "merged_pcd = o3d.geometry.PointCloud()\n",
    "time_rec = []\n",
    "lateral_err = []\n",
    "# Loop through all frames\n",
    "for i, frame_file in enumerate(frames_dir):\n",
    "    frame_pcd = load_point_cloud(os.path.join(dataset_dir, 'frames', frame_file))\n",
    "    frame_points = np.asarray(frame_pcd.points)\n",
    "\n",
    "    start_time = time.time()\n",
    "    transformation = ndt_registration(frame_points, voxel_mean, voxel_covariance, voxel_size, min_bounds)\n",
    "    end_time = time.time()\n",
    "\n",
    "    aligned_filename = os.path.join(dataset_dir, 'aligned_frames', f'aligned_{frame_file}.pcd')\n",
    "    frame_pcd.transform(transformation)\n",
    "    merged_pcd += frame_pcd\n",
    "\n",
    "\n",
    "    # Calculate lateral error (example calculation)\n",
    "    gt_transform = ground_truth.iloc[i]\n",
    "    translation_gt = gt_transform[[' x', ' y', ' z']].values\n",
    "    rpy_gt = gt_transform[[' roll', ' pitch', ' yaw']].values\n",
    "    rotation_gt = rpy_to_rotation_matrix(*rpy_gt)\n",
    "\n",
    "    gt_transform_matrix = np.eye(4)\n",
    "    gt_transform_matrix[:3, :3] = rotation_gt\n",
    "    gt_transform_matrix[:3, 3] = translation_gt\n",
    "\n",
    "    # Compute the lateral error (difference in transformation)\n",
    "    difference = np.linalg.norm(transformation[:3, 3] - gt_transform_matrix[:3, 3])\n",
    "\n",
    "    computation_time = float(\"{:.2f}\".format(end_time - start_time))\n",
    "    time_rec.append(computation_time)\n",
    "\n",
    "    lat_err = float(\"{:.2f}\".format(difference))\n",
    "    lateral_err.append(lat_err)\n",
    "\n",
    "    print(f\"Frame {i}/{len(frames_dir)}: Lateral error = {difference:.2f} meters, Computation time = {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # Update initial transformation for next iteration (could be based on odometry or other)\n",
    "    initial_transformation = transformation\n",
    "\n",
    "    # Optional: Compare transformations\n",
    "    compare_transformations(transformation, gt_transform)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"----->>>>>>>>>>  Mean time per frame: {np.mean(time_rec):.2f} s\")\n",
    "print(f\"----->>>>>>>>>>  Maximal time per frame: {np.max(time_rec):.2f} s\")\n",
    "print(f\"----->>>>>>>>>>  Mean lateral error: {np.mean(lateral_err):.2f} m\")\n",
    "print(f\"----->>>>>>>>>>  Maximal lateral error: {np.max(lateral_err):.2f} m\")\n",
    "\n",
    "# Optional: Voxel downsample the merged point cloud to reduce noise and size\n",
    "merged_pcd = merged_pcd.voxel_down_sample(voxel_size=0.05)\n",
    "\n",
    "# Save the merged point cloud\n",
    "merged_filename = os.path.join(dataset_dir, 'merged_map.pcd')\n",
    "o3d.io.write_point_cloud(merged_filename, merged_pcd)\n",
    "\n",
    "\n",
    "#pcd = o3d.io.read_point_cloud(\"/home/ari/Workplace/JKU/SEM_2/Autonomous_sys/Project3/Autonomous_vehicles_ue_3/localization/Project/ICP/aligned_car.pcd\")\n",
    "#o3d.visualization.draw_geometries([pcd])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of ICP and NDT Algorithms\n",
    "\n",
    "#### Error Analysis\n",
    "\n",
    "**ICP Mean lateral error:** 0.07 m\n",
    "**NTD Mean lateral error:** 94.98 m\n",
    "\n",
    "**ICP Maximal lateral error:** 0.07 m\n",
    "**NTD Maximal lateral error:** 182.41 m\n",
    "\n",
    "**TODO**: Update if NDT Code is updated\n",
    "We observed lower error values in the ICP implementation compared to the NTD implementation. \n",
    "\n",
    "\n",
    "\n",
    "#### Computation Time\n",
    "\n",
    "**ICP Mean time per frame:** 0.12 s, \n",
    "**NTD Mean time per frame:** 0.08 s, \n",
    "\n",
    "**ICP Maximal time per frame:** 0.59 s\n",
    "**NTD Maximal time per frame:** 0.35 s\n",
    "\n",
    "In our project NDT shows advantages in terms of computational efficiency, requiring less time per frame on average and exhibiting lower maximum computation times. Algorithmic complexity also favors NDT in terms of scalability and computational load, especially in larger datasets or real-time applications. The observed superior computational efficiency is also confirmed by the literature: \n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8906233/#acm213521-bib-0017\n",
    "https://publish.mersin.edu.tr/index.php/igd/article/view/387\n",
    " **TODO: Add proper citation** \n",
    "\n",
    "\n",
    "### Complexity\n",
    "\n",
    "TODO: Compare the complexity of the algorithms\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
