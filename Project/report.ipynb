{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "Our project's objective is to achieve precise localization for a car driving within a simulated environment. The task involves ensuring that the car can travel a minimum distance of 170 meters from its starting position while minimizing the distance pose error. Practically, this error should not exceed 1.2 meters.\n",
    "\n",
    "To accomplish this, we will utilize point clouds extracted from a simulated car equipped with a LIDAR sensor, which provides regular LIDAR scans. Additionally, we have an existing point cloud map, which has been extracted from the CARLA simulator. Localization will be achieved by performing point registration matching between the real-time LIDAR scans and the pre-existing point cloud map. For this we implemented the ICP and NTD localization algorithm respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "TODO: Write explaination about the steps to run the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICP Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iterative Closest Point (ICP) algorithm is used to align two sets of points, typically referred to as the source points \\( P \\) and the target points \\( Q \\). In the context of a self-driving car, the target points \\( Q \\) are provided by a map, while the source points \\( P \\) come from the car's sensors. The goal is to transform the car's sensor data so it aligns with the map coordinates, specifically aligning the x and y coordinates through translation and rotation. This is achieved by minimizing the sum of squared errors between the points. This is achieved iteratively. \n",
    "\n",
    "One iteration includes the following steps:\n",
    "\n",
    "1. Calculate the centroids (mean points) of both \\( P \\) and \\( Q \\).\n",
    "\n",
    "2. Create a new set \\( X \\) by subtracting each point in \\( P \\) by the centroid of \\( P \\), and create a new set \\( Y \\) by subtracting each point in \\( Q \\) by the centroid of \\( Q \\).\n",
    "\n",
    "3. Compute the cross covariance matrix \\( S \\) by multiplying \\( X \\) with the transpose of \\( Y \\).\n",
    "\n",
    "4. Perform SVD on the matrix \\( S \\) to obtain matrices \\( U \\), \\( \\Sigma \\), and \\( V^T \\).\n",
    "\n",
    "5. \n",
    "    - The rotation matrix \\( R \\) is calculated as \\( R = VU^T \\).\n",
    "    - The translation vector \\( t \\) is calculated as \\( t = \\mu_Q - R\\mu_P \\), where \\( \\mu_Q \\) and \\( \\mu_P \\) are the centroids of \\( Q \\) and \\( P \\), respectively.\n",
    "\n",
    "6. We then iterativeley repeat the above steps until the sum of squared errors is minimized to a satisfactory level.\n",
    "\n",
    "By iteratively updating the transformation, the ICP algorithm aligns the source points \\( P \\) to the target points \\( Q \\), effectively mapping the car's sensor data to the map coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module configuration\n",
    "\n",
    "TODO: Explain the module configuration and selected parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset performance metrics\n",
    "\n",
    "On the provided dataset, our model demonstrated the following performance metrics:\n",
    "\n",
    "- **Mean time per frame**: 0.12 seconds\n",
    "- **Maximal time per frame**: 0.59 seconds\n",
    "- **Mean lateral error**: 0.07 meters\n",
    "- **Maximal lateral error**: 0.07 meters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# TODO: Update paths, remove final part about visualization?\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#\n",
    "# Paths\n",
    "map_path = \"localization/dataset/map.pcd\"\n",
    "frames_path = \"localization/dataset/frames\"\n",
    "ground_truth_path = \"localization/dataset/ground_truth.csv\"\n",
    "\n",
    "# point cloud\n",
    "map_cloud = o3d.io.read_point_cloud(map_path)\n",
    "\n",
    "# ground truth\n",
    "ground_truth = pd.read_csv(ground_truth_path)\n",
    "\n",
    "extracted_cars = []\n",
    "\n",
    "#frame\n",
    "for i in range(len(ground_truth)):\n",
    "    frame_file = os.path.join(frames_path, f\"frame_{i}.pcd\")\n",
    "\n",
    "    if not os.path.exists(frame_file):\n",
    "        print(f\"Frame file {frame_file} not found.\")\n",
    "        continue\n",
    "\n",
    "    frame_cloud = o3d.io.read_point_cloud(frame_file)\n",
    "    frame_cloud_down = frame_cloud.voxel_down_sample(voxel_size=0.05)\n",
    "\n",
    "    # ground truth\n",
    "    x, y, z = ground_truth.iloc[i][1:4]\n",
    "    roll, pitch, yaw = np.deg2rad(ground_truth.iloc[i][4:7])\n",
    "\n",
    "    # transformation matrix\n",
    "    r = R.from_euler('xyz', [roll, pitch, yaw])\n",
    "    rotation_matrix = r.as_matrix()\n",
    "    translation_vector = np.array([x, y, z])\n",
    "    transformation_matrix = np.eye(4)\n",
    "    transformation_matrix[:3, :3] = rotation_matrix\n",
    "    transformation_matrix[:3, 3] = translation_vector\n",
    "\n",
    "    # transformation\n",
    "    frame_cloud_down.transform(transformation_matrix)\n",
    "    extracted_cars.append(frame_cloud_down)\n",
    "\n",
    "# ICP\n",
    "threshold = 0.1\n",
    "initial_transformation = np.eye(4)\n",
    "maximum_iteration = 100\n",
    "\n",
    "time_rec = []\n",
    "lateral_err = []\n",
    "\n",
    "\n",
    "for i, car_cloud in enumerate(extracted_cars):\n",
    "    start_time = time.time()\n",
    "\n",
    "    reg_p2l = o3d.pipelines.registration.registration_icp(\n",
    "        car_cloud, map_cloud, threshold, initial_transformation,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=maximum_iteration)\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_rec.append(end_time - start_time)\n",
    "\n",
    "    # registration error\n",
    "    lateral_error = o3d.pipelines.registration.evaluate_registration(\n",
    "        car_cloud, map_cloud, threshold, reg_p2l.transformation\n",
    "    ).inlier_rmse\n",
    "    lateral_err.append(lateral_error)\n",
    "\n",
    "    if lateral_error > 1.2:\n",
    "        print(f\"Lateral error ({lateral_error:.2f} m) is greater than the maximum allowed (1.2 m).\")\n",
    "        break\n",
    "\n",
    "    # initial transformation\n",
    "    initial_transformation = reg_p2l.transformation\n",
    "\n",
    "    print(f\"Processed {i + 1}/{len(extracted_cars)} frames.\")\n",
    "\n",
    "# results\n",
    "print(f\"----->>>>>>>>>>  Mean time per frame: {np.mean(time_rec):.2f} s\")\n",
    "print(f\"----->>>>>>>>>>  Maximal time per frame: {np.max(time_rec):.2f} s\")\n",
    "print(f\"----->>>>>>>>>>  Mean lateral error: {np.mean(lateral_err):.2f} m\")\n",
    "print(f\"----->>>>>>>>>>  Maximal lateral error: {np.max(lateral_err):.2f} m\")\n",
    "\n",
    "\n",
    "# aligned car point clouds\n",
    "merged_cloud = map_cloud\n",
    "for car_cloud in extracted_cars:\n",
    "    merged_cloud += car_cloud\n",
    "\n",
    "o3d.io.write_point_cloud(\"aligned_car.pcd\", merged_cloud)\n",
    "# # ######################################################### End of solution #########################################################\n",
    "\n",
    "# After saving the aligned point cloud, you can visualize it using the following code:\n",
    "# pcd = o3d.io.read_point_cloud(\"/home/ari/Workplace/JKU/SEM_2/Autonomous_sys/Project3/Localization_Project/localization/Project/ICP/aligned_car.pcd\")\n",
    "# o3d.visualization.draw_geometries([pcd])\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTD Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm explaination\n",
    "\n",
    "Unlike the Iterative Closest Point (ICP) algorithm, which minimizes point-to-point distances, NDT aligns a set of source points to a target point cloud by modeling the target as a set of normal distributions via Multivariate Normal Distribution. Since it is possible that we obtain probability density functions that dont represent our point cloud, we divide the space of the poinclouds into a grid of cells (also called voxels) and calculate the Multivariate Normal Distribution for each cell. \n",
    "\n",
    " #### TODO: Explain the steps in NTD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module configuration\n",
    "TODO: Explain the module configuration and selected parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset performance metrics\n",
    "TODO: Elaborate on the performance metrics attained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Copy the runable code for the NTD localization into this section\n",
    "\n",
    "\n",
    "import open3d as o3d\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class NDTRegistration:\n",
    "    def __init__(self, resolution):\n",
    "        self.resolution = resolution  # Voxel grid size\n",
    "\n",
    "    def voxel_index(self, point):\n",
    "        return np.floor(point / self.resolution).astype(np.int32)\n",
    "\n",
    "    def create_voxel_grid(self, point_cloud):\n",
    "        indices = np.apply_along_axis(self.voxel_index, 1, np.asarray(point_cloud.points))\n",
    "        voxel_dict = {}\n",
    "        for index, p in zip(indices, np.asarray(point_cloud.points)):\n",
    "            index_tuple = tuple(index)\n",
    "            if index_tuple not in voxel_dict:\n",
    "                voxel_dict[index_tuple] = []\n",
    "            voxel_dict[index_tuple].append(p)\n",
    "        return voxel_dict\n",
    "\n",
    "    def compute_voxel_stats(self, voxel_grid):\n",
    "        stats = {}\n",
    "        for voxel, points in voxel_grid.items():\n",
    "            if len(points) > 0:\n",
    "                points = np.array(points)\n",
    "                mean = np.mean(points, axis=0)\n",
    "                covariance = np.cov(points, rowvar=False)\n",
    "                stats[voxel] = (mean, covariance)\n",
    "        return stats\n",
    "\n",
    "    def register(self, source, target):\n",
    "        source_voxel = self.create_voxel_grid(source)\n",
    "        target_voxel = self.create_voxel_grid(target)\n",
    "\n",
    "        source_stats = self.compute_voxel_stats(source_voxel)\n",
    "        target_stats = self.compute_voxel_stats(target_voxel)\n",
    "        transformation = np.eye(4)\n",
    "        return transformation\n",
    "\n",
    "def load_ground_truth(file_path):\n",
    "    return pd.read_csv(file_path, delimiter=',')\n",
    "\n",
    "def load_pcd(frame_number, dataset_folder):\n",
    "    file_path = os.path.join(dataset_folder, f\"frames/frame_{frame_number}.pcd\")\n",
    "    if os.path.exists(file_path):\n",
    "        return o3d.io.read_point_cloud(file_path)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def transform_point_cloud(pcd, x, y, z, roll, pitch, yaw):\n",
    "    R = np.array([\n",
    "        [np.cos(yaw)*np.cos(pitch), np.cos(yaw)*np.sin(pitch)*np.sin(roll) - np.sin(yaw)*np.cos(roll), np.cos(yaw)*np.sin(pitch)*np.cos(roll) + np.sin(yaw)*np.sin(roll)],\n",
    "        [np.sin(yaw)*np.cos(pitch), np.sin(yaw)*np.sin(pitch)*np.sin(roll) + np.cos(yaw)*np.cos(roll), np.sin(yaw)*np.sin(pitch)*np.cos(roll) - np.cos(yaw)*np.sin(roll)],\n",
    "        [-np.sin(pitch), np.cos(pitch)*np.sin(roll), np.cos(pitch)*np.cos(roll)]\n",
    "    ])\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R\n",
    "    T[0, 3] = x\n",
    "    T[1, 3] = y\n",
    "    T[2, 3] = z\n",
    "    pcd.transform(T)\n",
    "    return pcd\n",
    "\n",
    "def main():\n",
    "    dataset_folder = 'localization/dataset'\n",
    "    ground_truth_file = 'localization/dataset/ground_truth.csv'\n",
    "    reference_pcd_path = 'localization/dataset/reference_map.pcd'  # Path to the reference point cloud\n",
    "\n",
    "    ground_truth = load_ground_truth(ground_truth_file)\n",
    "    ground_truth.columns = [col.strip().lower() for col in ground_truth.columns]\n",
    "    reference_pcd = o3d.io.read_point_cloud(reference_pcd_path)  # Load reference point cloud once\n",
    "\n",
    "    ndt = NDTRegistration(resolution=1.0)  # Initialize NDT registration\n",
    "\n",
    "    for index, row in ground_truth.iterrows():\n",
    "        frame_number = int(row['frame'])\n",
    "        source_pcd = load_pcd(frame_number, dataset_folder)\n",
    "        if source_pcd is not None:\n",
    "            transformed_pcd = transform_point_cloud(source_pcd, row['x'], row['y'], row['z'], row['roll'], row['pitch'], row['yaw'])\n",
    "            transformation_ndt = ndt.register(transformed_pcd, reference_pcd)\n",
    "            transformed_pcd.transform(transformation_ndt)  # Apply NDT transformation\n",
    "            o3d.visualization.draw_geometries([transformed_pcd, reference_pcd])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of both algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error through frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: compare the error through frames \n",
    "ICP: \n",
    "Mean lateral error: 0.07 m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: compare the maximum error of \n",
    "\n",
    "ICP: \n",
    "Maximal lateral error: 0.07 m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Compare the computation time \n",
    "\n",
    "ICP: \n",
    "Mean time per frame: 0.12 s\n",
    "Maximal time per frame: 0.59 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Compare the complexity of the algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
